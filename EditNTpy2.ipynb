{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EditNTpy2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cranial490/AgePrediction/blob/master/EditNTpy2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5NT_IwAWrEU",
        "colab_type": "code",
        "outputId": "17d65641-699c-4623-c21c-e67f20314490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        " !git clone https://github.com/adityakrgupta25/EditNTS.git\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%pwd\n",
        "%cd EditNTS\n",
        "%pwd\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EditNTS'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)\u001b[K\rremote: Counting objects:  33% (2/6)\u001b[K\rremote: Counting objects:  50% (3/6)\u001b[K\rremote: Counting objects:  66% (4/6)\u001b[K\rremote: Counting objects:  83% (5/6)\u001b[K\rremote: Counting objects: 100% (6/6)\u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 65 (delta 1), reused 2 (delta 0), pack-reused 59\u001b[K\n",
            "Unpacking objects: 100% (65/65), done.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/EditNTS/EditNTS/EditNTS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'/content/EditNTS/EditNTS/EditNTS'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HIA_xmhPZ1",
        "colab_type": "text"
      },
      "source": [
        "Go to paneer lasgna gmail account and get the code from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3hCthFppPDs",
        "colab_type": "text"
      },
      "source": [
        "Link to SIF notebook:\n",
        "https://colab.research.google.com/drive/1Bz_IdMkmiEPLWRqShNJ_l_t8p8ZEvFg5?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo3qvSgeW9pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ref: https://towardsdatascience.com/upgrade-your-memory-on-google-colab-for-free-1b8b18e8791d\n",
        "# didnt work, other hacks: https://www.analyticsvidhya.com/blog/2020/04/5-amazing-google-colab-hacks-you-should-try-today/\n",
        "#a = []\n",
        "#while(1):\n",
        "#    a.append('1')\n",
        "# https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting?answertab=oldest#tab-top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DGkZ8c3gIo2",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/c/bengaliai-cv19/discussion/133857\n",
        "\n",
        "https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting?answertab=oldest#tab-top\n",
        "Use this to avoid disconnection.\n",
        "```\n",
        "function ClickConnect(){\n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u__lbwHergf",
        "colab_type": "text"
      },
      "source": [
        "Trying to setup GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPE4kO5JGi11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get glove word embedding.\n",
        "# Todo -> Store this dataset in drive and copy from there.\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove*.zip\n",
        "# !cp glove.6B.100d.txt vocab_data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDov5ZJ-Svkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Get Wikipedia simple English\n",
        "# !wget https://fileserver.ukp.informatik.tu-darmstadt.de/UKP_Webpage/DATA/PWKP_108016.tar.gz\n",
        "# !tar -xvf  \"PWKP_108016.tar.gz\" -C 'vocab_data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1fGeBORboms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup NLTK something\n",
        "%%capture\n",
        "import nltk\n",
        "nltk.download('all') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7tXQR5qsHct",
        "colab_type": "text"
      },
      "source": [
        "So data_preprocess mein two things needs to be done:\n",
        "1. process_raw_data(comp_text, simp_text) -> Yaha we need to pass a list of comp_text and simp_text (maybe this we'll fetch from that wikipedia link you were talking about). It will return a df (let's call it proccessed_df)\n",
        "2. editnet_data_to_editnetID(df, output_path) -> Yaha we need to pass proccessed_df from earlier, and some output_path where training set will be stored I think\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAoMwnS6pKPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from data_preprocess import *\n",
        "# lines = []\n",
        "# with open(\"../drive/My Drive/EditNTS/data/vocab_data/PWKP_108016\", 'r+') as f:\n",
        "#   lines = f.readlines()\n",
        "\n",
        "# comp_text = []\n",
        "# simp_text = []\n",
        "# comp_text_temp = \"\"\n",
        "# simp_text_temp = \"\"\n",
        "# comp_text_flag = False\n",
        "# for line in lines:\n",
        "#   line = line.replace('\\n', ' ')\n",
        "#   if line.strip() == '':\n",
        "#     comp_text.append(comp_text_temp)\n",
        "#     simp_text.append(simp_text_temp)\n",
        "#     comp_text_temp = \"\"\n",
        "#     simp_text_temp = \"\"\n",
        "#     comp_text_flag = False\n",
        "#   else:\n",
        "#       if comp_text_flag:\n",
        "#         simp_text_temp += line\n",
        "#       else:\n",
        "#         comp_text_temp += line\n",
        "#         comp_text_flag = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HncAQF96z7Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import data\n",
        "# def process_raw_data(comp_txt, simp_txt):\n",
        "#     comp_txt = [line.lower().split() for line in comp_txt]\n",
        "#     simp_txt = [line.lower().split() for line in simp_txt]\n",
        "#     # df_comp = pd.read_csv('data/%s_comp.csv'%dataset,  sep='\\t')\n",
        "#     # df_simp= pd.read_csv('data/%s_simp.csv'%dataset,  sep='\\t')\n",
        "#     assert len(comp_txt) == len(simp_txt)\n",
        "#     df = pd.DataFrame(\n",
        "#                         {'comp_tokens': comp_txt,\n",
        "#                          'simp_tokens': simp_txt,\n",
        "#                         })\n",
        "#     def add_edits(df):\n",
        "#         \"\"\"\n",
        "#         :param df: a Dataframe at least contains columns of ['comp_tokens', 'simp_tokens']\n",
        "#         :return: df: a df with an extra column of target edit operations\n",
        "#         \"\"\"\n",
        "#         comp_sentences = df['comp_tokens'].tolist()\n",
        "#         simp_sentences = df['simp_tokens'].tolist()\n",
        "#         pair_sentences = list(zip(comp_sentences,simp_sentences))\n",
        "\n",
        "#         edits_list = [sent2edit(l[0],l[1]) for l in pair_sentences] # transform to edits based on comp_tokens and simp_tokens\n",
        "#         df['edit_labels'] = edits_list\n",
        "#         return df\n",
        "\n",
        "#     def add_pos(df):\n",
        "#         src_sentences = df['comp_tokens'].tolist()\n",
        "#         pos_sentences = [pos_tag(sent) for sent in src_sentences]\n",
        "#         df['comp_pos_tags'] = pos_sentences\n",
        "#         vocab_path='vocab_data/'\n",
        "#         pos_vocab = data.POSvocab(vocab_path)\n",
        "#         pos_ids_list = []\n",
        "#         for sent in pos_sentences:\n",
        "#             pos_ids = [pos_vocab.w2i[w[1]] if w[1] in pos_vocab.w2i.keys() else pos_vocab.w2i[UNK] for w in sent]\n",
        "#             pos_ids_list.append(pos_ids)\n",
        "#         df['comp_pos_ids'] = pos_ids_list\n",
        "#         return df\n",
        "\n",
        "#     df = add_pos(df)\n",
        "#     df = add_edits(df)\n",
        "#     return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3F0dfjsMo8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.head(5)\n",
        "# processed_df = process_raw_data(comp_text, simp_text)\n",
        "# print(processed_df.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPucJ-Lni79F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# msk = np.random.rand(len(processed_df)) < 0.8\n",
        "# train_set = processed_df[msk]\n",
        "# eval_set = processed_df[~msk]\n",
        "# print(len(train_set))\n",
        "# print(len(eval_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAogSDsL4seS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tqdm import tqdm\n",
        "# def editnet_data_to_editnetID_mod(df,output_path):\n",
        "#     \"\"\"\n",
        "#     this function reads from df.columns=['comp_tokens', 'simp_tokens', 'edit_labels','comp_pos_tags','comp_pos_ids']\n",
        "#     and add vocab ids for comp_tokens, simp_tokens, and edit_labels\n",
        "#     :param df: df.columns=['comp_tokens', 'simp_tokens', 'edit_labels','comp_pos_tags','comp_pos_ids']\n",
        "#     :param output_path: the path to store the df\n",
        "#     :return: a dataframe with df.columns=['comp_tokens', 'simp_tokens', 'edit_labels',\n",
        "#                                             'comp_ids','simp_id','edit_ids',\n",
        "#                                             'comp_pos_tags','comp_pos_ids'])\n",
        "#     \"\"\"\n",
        "#     out_list = []\n",
        "#     vocab = data.Vocab()\n",
        "#     vocab.add_vocab_from_file('./vocab_data/vocab.txt', 30000)\n",
        "\n",
        "#     def prepare_example(example, vocab):\n",
        "#         \"\"\"\n",
        "#         :param example: one row in pandas dataframe with feild ['comp_tokens', 'simp_tokens', 'edit_labels']\n",
        "#         :param vocab: vocab object for translation\n",
        "#         :return: inp: original input sentence,\n",
        "#         \"\"\"\n",
        "#         comp_id = np.array([vocab.w2i[i] if i in vocab.w2i.keys() else vocab.w2i[UNK] for i in example['comp_tokens']])\n",
        "#         simp_id = np.array([vocab.w2i[i] if i in vocab.w2i.keys() else vocab.w2i[UNK] for i in example['simp_tokens']])\n",
        "#         edit_id = np.array([vocab.w2i[i] if i in vocab.w2i.keys() else vocab.w2i[UNK] for i in example['edit_labels']])\n",
        "#         return comp_id, simp_id, edit_id  # add a dimension for batch, batch_size =1\n",
        "    \n",
        "#     with tqdm(total=len(list(df.iterrows()))) as pbar:\n",
        "#       for i,example in df.iterrows():\n",
        "#           comp_id, simp_id, edit_id = prepare_example(example,vocab)\n",
        "#           ex=[example['comp_tokens'], comp_id,\n",
        "#           example['simp_tokens'], simp_id,\n",
        "#           example['edit_labels'], edit_id,\n",
        "#           example['comp_pos_tags'],example['comp_pos_ids']\n",
        "#           ]\n",
        "#           out_list.append(ex)\n",
        "#           pbar.update(1)\n",
        "#     outdf = pd.DataFrame(out_list, columns=['comp_tokens','comp_ids', 'simp_tokens','simp_ids',\n",
        "#                                               'edit_labels','new_edit_ids','comp_pos_tags','comp_pos_ids'])\n",
        "#     outdf.to_pickle(output_path)\n",
        "    # print('saved to %s'%output_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjIGbeXhmat3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VHyMO2HfMBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# editnet_data_to_editnetID_mod(train_set, \"../drive/My Drive/EditNTS/data/vocab_data/train.df.filtered.pos\")\n",
        "# editnet_data_to_editnetID_mod(eval_set, \"../drive/My Drive/EditNTS/data/vocab_data/val.df.filtered.pos\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-GgU4UgHbmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python main.py --data_path \"../drive/My Drive/EditNTS/data/vocab_data/\" --vocab_path \"../drive/My Drive/EditNTS/data/vocab_data/\" --device 0 --store_dir '../drive/My Drive/EditNTS/models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gXrJd5Y5kg-",
        "colab_type": "code",
        "outputId": "e2bd8d18-bd74-45b1-8bc9-f066a40c055f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "torch.cuda.device_count()\n",
        "\n",
        "#torch.cuda.set_device('cuda:0p\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.current_device())\n",
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AOaGPXl2o5b",
        "colab_type": "code",
        "outputId": "10c8f0ec-b3a9-42c3-c09c-1697f9d0af8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "!python main.py --data_path \"../drive/My Drive/EditNTS/data/vocab_data/\" --vocab_path \"../drive/My Drive/EditNTS/data/vocab_data/\" --device 0 --store_dir '../drive/My Drive/EditNTS/models'  --load_model '../drive/My Drive/EditNTS/models/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/EditNTS\n",
            "**********\n",
            "read 30006 words from vocab file\n",
            "Loading Glove embeddings\n",
            "29730 words out of 30006 has embeddings in the glove file\n",
            "**********\n",
            "Namespace(batch_size=32, data_path='../drive/My Drive/EditNTS/data/vocab_data/', device=0, epochs=50, hidden=200, load_model='../drive/My Drive/EditNTS/models/', lr=0.0001, max_seq_len=100, store_dir='../drive/My Drive/EditNTS/models', vocab_path='../drive/My Drive/EditNTS/data/vocab_data/', vocab_size=30000)\n",
            "generating config\n",
            "init editNTS model\n",
            "load pre-trained embeddings\n",
            "load edit_net for further training from %s ../drive/My Drive/EditNTS/models/checkpoints/2020_06_13_10_46_18\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'editnts.EditNTS' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python2.7/dist-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "the fking model is, <class 'editnts.EditNTS'>\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 255, in <module>\n",
            "    main()\n",
            "  File \"main.py\", line 247, in main\n",
            "    training(edit_net, args.epochs, args, vocab)\n",
            "  File \"main.py\", line 130, in training\n",
            "    output = edit_net(org, out, org_ids, org_pos,simp_ids)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/EditNTS/editnts.py\", line 427, in forward\n",
            "    print(\"hidden_org size:\",hidden_org.size())\n",
            "AttributeError: 'tuple' object has no attribute 'size'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVO08pbDmxId",
        "colab_type": "code",
        "outputId": "416d9c5a-49fd-40f4-988b-c0dca7ac4b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import data\n",
        "from checkpoint import Checkpoint\n",
        "from editnts import EditNTS\n",
        "from evaluator import Evaluator\n",
        "torch.manual_seed(233)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\n",
        "\n",
        "parser = argparse.ArgumentParser()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b20d9c75ea24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m parser.add_argument('--data_path', type=str,dest='data_path',\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/ml/ydong26/data/EditNTS_data/editnet_data/%s/'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                     help='Path to train vocab_data')\n\u001b[1;32m     22\u001b[0m parser.add_argument('--store_dir', action='store', dest='store_dir',\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz-DYiKQtuB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import data\n",
        "dataset = data.Dataset('../drive/My Drive/EditNTS/data/vocab_data/' + 'val.df.filtered.pos')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln_k9YIWuYyX",
        "colab_type": "code",
        "outputId": "9bf2efed-4060-45a2-d865-673e42539bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/EditNTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqypJ1VMnJSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import data\n",
        "from checkpoint import Checkpoint\n",
        "from editnts import EditNTS\n",
        "from evaluator import Evaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zSF_nzenOZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = data.Vocab()\n",
        "vocab_path = '../drive/My Drive/EditNTS/data/vocab_data/'\n",
        "vocab.add_vocab_from_file(vocab_path+'vocab.txt', args.vocab_size)\n",
        "vocab.add_embedding(gloveFile=args.vocab_path+'glove.6B.100d.txt')\n",
        "    pos_vocab = data.POSvocab(args.vocab_path) #load pos-tags embeddings\n",
        "    print('*' * 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}